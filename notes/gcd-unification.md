# GCD and unification

Conor McBride wrote:

> I would love love love to fix my first year course so we start with spuds-and-lemons gcd, and end with unification.

When I asked them about this connection, they responded:

> Every monoid (multiplication of numbers or substitution of terms) induces a slice category ("this is a multiple of that" or "this is an instance of that") which raises the question of whether those slice categories have products, i.e. whether the original monoids have pullbacks ("this is the greatest common divisor" or "this is the most general unifier")

I found this absolutely intriguing, because in the game of the Six Degrees of
the Stern-Brocot tree, it has the potential for being a relatively simple
connection to logic.

Their response is anything but a detailed exposition of the core ideas, but I
think it should be enough for me to work the details out.

## Pullbacks

My main challenge here is that I don't have any real, deep understanding of
category theory, though my recent interest in exact sequences has been helping
me to appreciate the subject more easily.

I started thinking about this in the wrong category, specifically the category
of groups, because it's a category I'm relatively familiar with.  Namely, I
started considering pullbacks on nZ for n in N,  which are the subgroups of Z.

1.  While this doesn't seem to be fertile ground for properly understanding and
    appreciating pullbacks, it was enough to start to see how some of it should
    work, and build confidence that I would be able to fill in the details.

2.  Part of the challenge here is that all subgroups of Z are isomorphic to each
    other. So while I think the approach is promising, I'm clearly working in
    the wrong category.

3.  This exercise did help clarify and solidify my growing realization that part
    of my difficulty with category theory is that I was conflating objects
    with groups a little too closely, thus causing some confusion along the
    "sense and reference" meanings of names.

    For example `Z` and `2Z` are often considered to be distinct objects in the
    category of groups, whereas traditional abstract algebra tends to play
    fast-and-loose with isomorphism, often conflating `Z` and `2Z`.

So first I need to switch to the category of monoids, which I don't know quite
as well, but it should be fairly straightforward for me to figure out. I also
need to figure out a bit about slice categories, which I know nothing about.

## A digression about submonoids

Characterizing the submonoids of `<N, +>` is substantially more complicated
than the subgroups of `<Z, +>`.

For starters, `nN` is a submonoid of `N` for each `n ∈ N`. This characterizes
all the submonoids of N generated by the forward orbit of a single natural
number. However, each of these submonoids have further submonoids, which
consist of all of the elements of `nN` with a finite number of exceptions.

These submonoids result from using multiple generators. The fact that these
submonoids remove a finite number of elements from `nN` is the Frobenius coin
problem.

In particular, these submonoids consist of exactly of the multiples of the
greatest common divisor of the generators, for some sufficiently large multiple.

**Theorem:** All the submonoids of `N` be generated from taking the transitive
closure of a finite number of starting elements.

**Sketch of proof:** Other than the trivial submonoid, any submonoid `M` of `N`
has to have an infinite number of elements, which are obviously generated by `M`
itself. No generating set needs to include the identity, so we'll consider or
initial generating set `G_0 = M \ {0}`, which trivially generates `M`.

Pick the smallest element in `G_0`, and call it `n_0`. Then `n_0 N` is a
submonoid of `M`, and we can remove all multiples of `n_0` from our next
generating set `G_1 = G_0 \ n_0 (N + 1)`.

Essentially, we repeat this process, though we need to fix up the induction
step to get it right in the general case. At this point, pick the smallest
element of `G_1`, call it `n_1`, and then we remove all the elements generated
by `n_0` and `n_1`.  Thus `G_2 = G_1 \ <n_0, n_1>`.

What is generated by `<n_0, n_1> includes `d (N + g(n_0 / d , n_1 / d) + 1)`.
Here `d` is the `gcd(n_0, n_1)`, and `g` is the Frobenius function.

As gcd is a monotonically non-increasing function, this procedure eventually
reduces some G_k to a finite set. (why? I'm missing details here.) After some
number of additional steps, we can derive a complete set of minimal generators
`{ n_i | i ∈ {0..j} }` with `G_j` being the empty set.

To what extent can this analysis be generalized?  I'm especially interested
in submonoids of `SL(2,N)`, `GL(2,Z)` and friends.

## slice categories

Okay, now that I feel like I have a better understanding of the category of
monoids with this warm-up exercise, I also need to understand slice categories.

Really, understanding slice categories seems to be my biggest challenge at the
moment, though I'm sure I need to understand pullbacks better as well.

Slice categories remind me of continuations. Clearly, there is a fair bit of
overlap between the concepts. I'm somewhat confident there are differences as
well. One thing I notice is that the "type" of slice categories moves some
variables around relative to the "type" of continuations, but the underlying
structure is otherwise very similiar.

Being able to nail down this relationship should be interesting. I do hope to
eventually have continuations comfortably inside my Six Degrees of the
Stern-Brocot tree, as I've often played with both but have never understood an
interesting connection between the two.

## a digression about commutativity

In the readme for this repository, I mentioned in passing that `SL(2,N)` never
commutes. That's not quite true.

It is true that if you write something down in the Stern-Brocot representation
`<L,R>`, then any change in the bitstring (including length) results in a
different element. So it's about as close to "never" commuting as you can get
using "nice" non-commutative operators, i.e. those operators that can be
expressed using matrix arithmetic.  (Hmm... is there anything that is
associative and non-commutative that can't be expressed in matrix arithmetic?
I suspect not, though I'd of course love to have a relatively simple
counterexample to pick out of my hat.)

When I wrote the comment, my tacit understanding was that in SL(2,N),
`x * y = y * x` implies `x = y`. This is true if x and y have the same length,
but in general, `x * y = y * x` if and only if `x = c^i` and `y = c^j` for some
`c ∈ SL(2,N)` and `i, j ∈ N`.

The proof essentially uses the greatest common divisor algorithm and
unification... which is a bit of a "Yo Dawg" moment.

Xzibit memes aside, if `x` and `y` commute, and their Stern-Brocot
representations are of equal length, then `x` must equal `y` because `SL(2,N)`
is a tree isomorphic to binary bitstrings.

If `x` and `y` are of unequal length, then without loss of generality, assume
`x` is shorter than `y`. By repeatedly swapping `x` and `y`, we can find that
`y` must be equal to `x^n` times some (possibly empty) prefix of `x`.

If that prefix is empty, then we are done, and `c` is equal to our current
version of `x`. Otherwise, we run our GCD/unification algorithm on `x` and that
prefix of `x`.  Eventually this process will terminate, as the total length
of the arguments is decreasing.

Now, is this observation an accident, or is there something deeper afoot here?

I mean, this GCD-like algorithm computes something seemingly very different
than the conventional GCD algorithm, right? Or is there a less-obvious
connection between... doing what exactly?  Finding the "GCD" of two
positive *fractions*??! (But only if their Stern-Brocot representations
commute?!)

Interestingly, by the Lagrange-Euler theorem, the elements in `SL(2,N)`
that commute with a given element forms a sequence that approximates
quadratic irrationals very efficiently.  Thus, any two elements that commute
are reasonable approximations of relatively "small" quadratic irrationals.

(As the quadratic irrationals form an everywhere-dense set, saying any
particular rational number is an excellent approximation of a quadratic
irrational is not a very interesting statement, as it's true of *every*
rational number. Thus, turning this qualitative observation into a useful
quantitative theorem requires explication. Surely this already corresponds to
something reasonably well known?)

### "never" commutative

My previous misunderstanding does raise an interesting question: how close
can we get to being "never" commutative in various kinds of algebraic
structures?

Given any associative, non-commutative binary operator, and any element `c`
within the domain of that operator, then `c^i` and `c^j` always commute for
all positive integers `i` and `j`.

Are there any "interesting" binary operators where the proposition
"if `x` and `y` commute, then `x` = `y` or `x` = 1 or `y` = 1" also holds true?

Well, then we'd need `c^i` = `c^j` for all elements `c`, and all positive
integers `i` and `j`. Thus all elements in the domain must be idempotents.

If `c` has an inverse, then `c` must be the identity element. If the binary
operator is part of a group, then there must only be one element, because
all elements would then have inverses. Thus in any non-trivial group, there
must be non-trivial examples where `x` and `y` commute but `x ≠ y` and `x ≠ 1`
and `y ≠ 1`.

Because `SL(2,N)` is a submonoid of `SL(2,Z)`, this argument should apply to
`SL(2,N)` as well.

But then, can't any monoid be extended into a group? I think the term is
"enveloping group", e.g. "universal enveloping group". I've not thought about
or studied this particular concept in depth. I think any enveloping group should
work, not just the universal enveloping group.

Would this then imply that *any* non-trivial monoid has examples where
`x * y = y * x` but also `x ≠ y` and `x ≠ 1` and `y ≠ 1`?

One possible complication is that a monoid can actually be *larger* than its
universal enveloping group. Basically, once you allow inverses, multiple
monoid elements can collapse into the same element. Assuming that my original
intuition that if `M` is non-trivial and a submonoid of `G`, then `M` contains
non-trivial pairs of elements that commute.
